{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C - CartPole\n",
    "Implementation of the A2C RL Algorithm for the OpenAI's Gym environment CartPole-V1 (not in parallel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Fuction Estimator\n",
    "class Critic(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(units=32, input_shape=[4,], activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(units=8, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(units=1, activation='relu')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Value Fuction Estimator (q-network)\n",
    "class Actor(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(units=128, input_shape=[4,], activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(units=8, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(units=2, activation='softmax')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = tf.convert_to_tensor(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer actor_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Episode 1 of 100 findished after 14 timesteps\n",
      "WARNING:tensorflow:Layer critic_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Episode 2 of 100 findished after 19 timesteps\n",
      "Episode 3 of 100 findished after 37 timesteps\n",
      "Episode 4 of 100 findished after 18 timesteps\n",
      "Episode 5 of 100 findished after 17 timesteps\n",
      "Episode 6 of 100 findished after 22 timesteps\n",
      "Episode 7 of 100 findished after 16 timesteps\n",
      "Episode 8 of 100 findished after 20 timesteps\n",
      "Episode 9 of 100 findished after 16 timesteps\n",
      "Episode 10 of 100 findished after 23 timesteps\n",
      "Episode 11 of 100 findished after 19 timesteps\n",
      "Episode 12 of 100 findished after 13 timesteps\n",
      "Episode 13 of 100 findished after 11 timesteps\n",
      "Episode 14 of 100 findished after 16 timesteps\n",
      "Episode 15 of 100 findished after 12 timesteps\n",
      "Episode 16 of 100 findished after 26 timesteps\n",
      "Episode 17 of 100 findished after 15 timesteps\n",
      "Episode 18 of 100 findished after 20 timesteps\n",
      "Episode 19 of 100 findished after 31 timesteps\n",
      "Episode 20 of 100 findished after 27 timesteps\n",
      "Episode 21 of 100 findished after 14 timesteps\n",
      "Episode 22 of 100 findished after 23 timesteps\n",
      "Episode 23 of 100 findished after 32 timesteps\n",
      "Episode 24 of 100 findished after 35 timesteps\n",
      "Episode 25 of 100 findished after 18 timesteps\n",
      "Episode 26 of 100 findished after 18 timesteps\n",
      "Episode 27 of 100 findished after 43 timesteps\n",
      "Episode 28 of 100 findished after 28 timesteps\n",
      "Episode 29 of 100 findished after 14 timesteps\n",
      "Episode 30 of 100 findished after 19 timesteps\n",
      "Episode 31 of 100 findished after 43 timesteps\n",
      "Episode 32 of 100 findished after 21 timesteps\n",
      "Episode 33 of 100 findished after 15 timesteps\n",
      "Episode 34 of 100 findished after 12 timesteps\n",
      "Episode 35 of 100 findished after 14 timesteps\n",
      "Episode 36 of 100 findished after 82 timesteps\n",
      "Episode 37 of 100 findished after 17 timesteps\n",
      "Episode 38 of 100 findished after 67 timesteps\n",
      "Episode 39 of 100 findished after 27 timesteps\n",
      "Episode 40 of 100 findished after 97 timesteps\n",
      "Episode 41 of 100 findished after 61 timesteps\n",
      "Episode 42 of 100 findished after 34 timesteps\n",
      "Episode 43 of 100 findished after 100 timesteps\n",
      "Episode 44 of 100 findished after 84 timesteps\n",
      "Episode 45 of 100 findished after 380 timesteps\n",
      "Episode 46 of 100 findished after 96 timesteps\n",
      "Episode 47 of 100 findished after 120 timesteps\n",
      "Episode 48 of 100 findished after 187 timesteps\n",
      "Episode 49 of 100 findished after 90 timesteps\n",
      "Episode 50 of 100 findished after 208 timesteps\n",
      "Episode 51 of 100 findished after 159 timesteps\n",
      "Episode 52 of 100 findished after 184 timesteps\n",
      "Episode 53 of 100 findished after 161 timesteps\n",
      "Episode 54 of 100 findished after 208 timesteps\n",
      "Episode 55 of 100 findished after 211 timesteps\n",
      "Episode 56 of 100 findished after 223 timesteps\n",
      "Episode 57 of 100 findished after 213 timesteps\n",
      "Episode 58 of 100 findished after 264 timesteps\n",
      "Episode 59 of 100 findished after 386 timesteps\n",
      "Episode 60 of 100 findished after 500 timesteps\n",
      "Episode 61 of 100 findished after 249 timesteps\n",
      "Episode 62 of 100 findished after 484 timesteps\n",
      "Episode 63 of 100 findished after 441 timesteps\n",
      "Episode 64 of 100 findished after 472 timesteps\n",
      "Episode 65 of 100 findished after 289 timesteps\n",
      "Episode 66 of 100 findished after 301 timesteps\n",
      "Episode 67 of 100 findished after 301 timesteps\n",
      "Episode 68 of 100 findished after 160 timesteps\n",
      "Episode 69 of 100 findished after 322 timesteps\n",
      "Episode 70 of 100 findished after 383 timesteps\n",
      "Episode 71 of 100 findished after 197 timesteps\n",
      "Episode 72 of 100 findished after 500 timesteps\n",
      "Episode 73 of 100 findished after 167 timesteps\n",
      "Episode 74 of 100 findished after 183 timesteps\n",
      "Episode 75 of 100 findished after 237 timesteps\n",
      "Episode 76 of 100 findished after 279 timesteps\n",
      "Episode 77 of 100 findished after 213 timesteps\n",
      "Episode 78 of 100 findished after 500 timesteps\n",
      "Episode 79 of 100 findished after 216 timesteps\n",
      "Episode 80 of 100 findished after 326 timesteps\n",
      "Episode 81 of 100 findished after 360 timesteps\n",
      "Episode 82 of 100 findished after 175 timesteps\n",
      "Episode 83 of 100 findished after 191 timesteps\n",
      "Episode 84 of 100 findished after 188 timesteps\n",
      "Episode 85 of 100 findished after 142 timesteps\n",
      "Episode 86 of 100 findished after 187 timesteps\n",
      "Episode 87 of 100 findished after 282 timesteps\n",
      "Episode 88 of 100 findished after 248 timesteps\n",
      "Episode 89 of 100 findished after 147 timesteps\n",
      "Episode 90 of 100 findished after 160 timesteps\n",
      "Episode 91 of 100 findished after 198 timesteps\n",
      "Episode 92 of 100 findished after 236 timesteps\n",
      "Episode 93 of 100 findished after 199 timesteps\n",
      "Episode 94 of 100 findished after 226 timesteps\n",
      "Episode 95 of 100 findished after 233 timesteps\n",
      "Episode 96 of 100 findished after 277 timesteps\n",
      "Episode 97 of 100 findished after 314 timesteps\n",
      "Episode 98 of 100 findished after 500 timesteps\n",
      "Episode 99 of 100 findished after 242 timesteps\n",
      "Episode 100 of 100 findished after 360 timesteps\n"
     ]
    }
   ],
   "source": [
    "# Prepare Tensorboard\n",
    "!rm -rf ./logs/\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "#%tensorboard --logdir logs/\n",
    "#tf.keras.backend.clear_session()\n",
    "# Initialize cart pole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "# Initialize model, loss and optimizer\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "actor_optimizer = tf.keras.optimizers.Adam()\n",
    "critic_optimizer = tf.keras.optimizers.Adam()\n",
    "mse = tf.keras.losses.MSE\n",
    "weighted_sparse_ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# Initialize replay memory\n",
    "observations = []\n",
    "# Set hyperparameters\n",
    "discount = 0.95\n",
    "max_time_steps = 500\n",
    "num_episodes = 100\n",
    "\n",
    "step = 0\n",
    "# Run for agent and environment for num_episodes\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    #breakpoint()\n",
    "    \n",
    "    # Agent has 500 trials at max, if it does not fail beforehand\n",
    "    for t in range(max_time_steps):\n",
    "        env.render()\n",
    "        # Compute action\n",
    "        state = np.reshape(state, [1,4])\n",
    "        probabilities = actor(state)\n",
    "        action = tf.random.categorical(tf.math.log(probabilities), 1)\n",
    "        # Execute action and store action, state and reward\n",
    "        next_state, reward, done, info = env.step(int(action))\n",
    "        observations.append((state, action, reward))\n",
    "        state = next_state\n",
    "        \n",
    "        # Interrupt the trial if the agent fails\n",
    "        if done:\n",
    "            break\n",
    "        step += 1\n",
    "        \n",
    "    print(f\"Episode {i_episode + 1} of {num_episodes} findished after {t+1} timesteps\")\n",
    "        \n",
    "    # Store losses temporary\n",
    "    losses = []\n",
    "    # Initialize variable for the estimated return\n",
    "    estimated_return = 0 if done else critic(next_state)\n",
    "    \n",
    "    # Iterate over taken actions and observed states and rewards\n",
    "    observations.reverse()\n",
    "    for state, action, reward in observations:\n",
    "        # Compute estimated return\n",
    "        estimated_return = discount * estimated_return + reward\n",
    "        # Compute state value\n",
    "        state_v = critic(state)\n",
    "    \n",
    "        # Compute gradients for the actor (policy gradient)\n",
    "        # Maximize the estimated return\n",
    "        with tf.GradientTape() as actor_tape:\n",
    "            logits = tf.math.log(actor(state))\n",
    "            advantages = estimated_return - int(state_v)\n",
    "            advantages = tf.cast([[advantages]], tf.float32)\n",
    "            action = tf.cast(action, tf.int32)\n",
    "            # Compute the actor loss (log part of the policy gradient)\n",
    "            actor_loss = weighted_sparse_ce(action, logits, sample_weight=advantages)\n",
    "            # Compute gradient with respect to the parameters of the actor            \n",
    "            policy_gradients = actor_tape.gradient(actor_loss, actor.trainable_variables)\n",
    "\n",
    "        # Compute gradients for the critic\n",
    "        # minimize MSE for the state value function\n",
    "        with tf.GradientTape() as critic_tape:\n",
    "            state_v = critic(state)\n",
    "            # Compute the loss\n",
    "            critic_loss = mse(estimated_return, state_v)\n",
    "            # Compute the gradient\n",
    "            critic_gradients = critic_tape.gradient(critic_loss, critic.trainable_variables)\n",
    "            #breakpoint()\n",
    "            # Accumulate gradients\n",
    "            #critic_gradients.append(gradients)\n",
    "            \n",
    "        # Apply gradients.\n",
    "        actor_optimizer.apply_gradients(zip(policy_gradients, actor.trainable_variables))\n",
    "        critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
    "        losses.append(actor_loss)\n",
    "\n",
    "    observations = []\n",
    "\n",
    "    # Store summary statistics\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('?b', tf.reduce_mean(losses), step=step)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
