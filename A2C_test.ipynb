{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C - CartPole\n",
    "Implementation of the A2C RL Algorithm for the OpenAI's Gym environment CartPole-V1 (not in parallel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Fuction Estimator\n",
    "class Critic(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(units=32, input_shape=[8,], activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(units=8, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(units=1, activation='relu')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Value Fuction Estimator (q-network)\n",
    "class Actor(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        # 64(share) -> 64(share) -> 32 -> 32 -> mu(tanh) [-1,1]\n",
    "        # 64(share) -> 64(share) -> 32 -> 32 -> sigma(sigmoid) [0,1]\n",
    "        self.sharedFC1 = tf.keras.layers.Dense(units=64, input_shape=[8,], activation='relu')\n",
    "        self.sharedFC2 = tf.keras.layers.Dense(units=64, activation='relu')\n",
    "        \n",
    "        self.muFC1 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        self.muFC2 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        \n",
    "        self.sigmaFC1 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        self.sigmaFC2 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        \n",
    "        \n",
    "        self.mu_out = tf.keras.layers.Dense(units=2, activation='tanh')\n",
    "        self.sigma_out = tf.keras.layers.Dense(units=2, activation='sigmoid')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = tf.convert_to_tensor(x)\n",
    "        x = self.sharedFC1(x)\n",
    "        x = self.sharedFC2(x)\n",
    "        \n",
    "        mu = self.muFC1(x)\n",
    "        mu = self.muFC2(mu)\n",
    "        mu = self.mu_out(mu)\n",
    "        \n",
    "        sigma = self.sigmaFC1(x)\n",
    "        sigma = self.sigmaFC2(sigma)\n",
    "        sigma = self.sigma_out(sigma)     \n",
    "        \n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor conversion requested dtype int32 for Tensor with dtype float32: <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.0586404], dtype=float32)>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c312602af445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mmainEngineAction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0msideEngineAction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainEngineAction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msideEngineAction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m#        mainEngineAction = np.reshape(action, (2,))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Execute action and store action, state and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1513\u001b[0m       ops.convert_to_tensor(\n\u001b[1;32m   1514\u001b[0m           \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"concat_dim\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m           dtype=dtypes.int32).get_shape().assert_has_rank(0)\n\u001b[0m\u001b[1;32m   1516\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1288\u001b[0m       raise ValueError(\n\u001b[1;32m   1289\u001b[0m           \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m           (dtype.name, value.dtype.name, value))\n\u001b[0m\u001b[1;32m   1291\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype int32 for Tensor with dtype float32: <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.0586404], dtype=float32)>"
     ]
    }
   ],
   "source": [
    "# Prepare Tensorboard\n",
    "!rm -rf ./logs/\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "#%tensorboard --logdir logs/\n",
    "#tf.keras.backend.clear_session()\n",
    "# Initialize cart pole environment\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "# Initialize model, loss and optimizer\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "actor_optimizer = tf.keras.optimizers.Adam()\n",
    "critic_optimizer = tf.keras.optimizers.Adam()\n",
    "mse = tf.keras.losses.MSE\n",
    "weighted_sparse_ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# Initialize replay memory\n",
    "observations = []\n",
    "# Set hyperparameters\n",
    "discount = 0.95\n",
    "max_time_steps = 500\n",
    "num_episodes = 100\n",
    "\n",
    "step = 0\n",
    "# Run for agent and environment for num_episodes\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    #breakpoint()\n",
    "    \n",
    "    # Agent has 500 trials at max, if it does not fail beforehand\n",
    "    for t in range(max_time_steps):\n",
    "#        env.render()\n",
    "        # Compute action\n",
    "        state = np.reshape(state, [1,8])\n",
    "        mu, sigma = actor(state)\n",
    "        \n",
    "        # sample two values from normal distribution\n",
    "        mainEngineAction = tf.random.normal((1,), mean=mu[0,0], stddev=sigma[0,0])\n",
    "        sideEngineAction = tf.random.normal((1,), mean=mu[0,1], stddev=sigma[0,1])\n",
    "        action = tf.concat(mainEngineAction, sideEngineAction, 0)\n",
    "        #        mainEngineAction = np.reshape(action, (2,))\n",
    "        # Execute action and store action, state and reward\n",
    "        print(mainEngineAction)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        observations.append((state, action, reward))\n",
    "        state = next_state\n",
    "        \n",
    "        # Interrupt the trial if the agent fails\n",
    "        if done:\n",
    "            break\n",
    "        step += 1\n",
    "        \n",
    "    print(f\"Episode {i_episode + 1} of {num_episodes} findished after {t+1} timesteps\")\n",
    "        \n",
    "    # Store losses temporary\n",
    "    losses = []\n",
    "    # Initialize variable for the estimated return\n",
    "    estimated_return = 0 if done else critic(next_state)\n",
    "    \n",
    "    # Iterate over taken actions and observed states and rewards\n",
    "    observations.reverse()\n",
    "    for state, action, reward in observations:\n",
    "        # Compute estimated return\n",
    "        estimated_return = discount * estimated_return + reward\n",
    "        # Compute state value\n",
    "        state_v = critic(state)\n",
    "    \n",
    "        # Compute gradients for the actor (policy gradient)\n",
    "        # Maximize the estimated return\n",
    "        with tf.GradientTape() as actor_tape:\n",
    "            logits = tf.math.log(actor(state))\n",
    "            advantages = estimated_return - int(state_v)\n",
    "            advantages = tf.cast([[advantages]], tf.float32)\n",
    "            action = tf.cast(action, tf.int32)\n",
    "            # Compute the actor loss (log part of the policy gradient)\n",
    "            actor_loss = weighted_sparse_ce(action, logits, sample_weight=advantages)\n",
    "            # Compute gradient with respect to the parameters of the actor            \n",
    "            policy_gradients = actor_tape.gradient(actor_loss, actor.trainable_variables)\n",
    "\n",
    "        # Compute gradients for the critic\n",
    "        # minimize MSE for the state value function\n",
    "        with tf.GradientTape() as critic_tape:\n",
    "            state_v = critic(state)\n",
    "            # Compute the loss\n",
    "            critic_loss = mse(estimated_return, state_v)\n",
    "            # Compute the gradient\n",
    "            critic_gradients = critic_tape.gradient(critic_loss, critic.trainable_variables)\n",
    "            #breakpoint()\n",
    "            # Accumulate gradients\n",
    "            #critic_gradients.append(gradients)\n",
    "            \n",
    "        # Apply gradients.\n",
    "        actor_optimizer.apply_gradients(zip(policy_gradients, actor.trainable_variables))\n",
    "        critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
    "        losses.append(actor_loss)\n",
    "\n",
    "    observations = []\n",
    "\n",
    "    # Store summary statistics\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('?b', tf.reduce_mean(losses), step=step)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(2,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
