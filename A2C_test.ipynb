{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C - CartPole\n",
    "Implementation of the A2C RL Algorithm for the OpenAI's Gym environment CartPole-V1 (not in parallel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[box2d] in /home/jonas/.venv/tensorflow-gpu/lib/python3.7/site-packages (0.16.0)\n",
      "Requirement already satisfied: six in /home/jonas/.venv/tensorflow-gpu/lib/python3.7/site-packages (from gym[box2d]) (1.13.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /home/jonas/.venv/tensorflow-gpu/lib/python3.7/site-packages (from gym[box2d]) (1.4.10)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in /home/jonas/.venv/tensorflow-gpu/lib/python3.7/site-packages (from gym[box2d]) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/jonas/.venv/tensorflow-gpu/lib/python3.7/site-packages (from gym[box2d]) (1.17.4)\n",
      "Requirement already satisfied: scipy in /home/jonas/.venv/tensorflow-gpu/lib/python3.7/site-packages (from gym[box2d]) (1.4.1)\n",
      "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in /home/jonas/.venv/tensorflow-gpu/lib/python3.7/site-packages (from gym[box2d]) (2.3.8)\n",
      "Requirement already satisfied: future in /home/jonas/.venv/tensorflow-gpu/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]) (0.18.2)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install 'gym[box2d]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Fuction Estimator\n",
    "class Critic(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(units=32, input_shape=[8,], activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(units=8, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(units=1, activation='relu')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Value Fuction Estimator (q-network)\n",
    "class Actor(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        # 64(share) -> 64(share) -> 32 -> 32 -> mu(tanh) [-1,1]\n",
    "        # 64(share) -> 64(share) -> 32 -> 32 -> sigma(sigmoid) [0,1]\n",
    "        self.sharedFC1 = tf.keras.layers.Dense(units=64, input_shape=[8,], activation='relu')\n",
    "        self.sharedFC2 = tf.keras.layers.Dense(units=64, activation='relu')\n",
    "        \n",
    "        self.muFC1 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        self.muFC2 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        \n",
    "        self.sigmaFC1 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        self.sigmaFC2 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        \n",
    "        \n",
    "        self.mu_out = tf.keras.layers.Dense(units=2, activation='tanh')\n",
    "        self.sigma_out = tf.keras.layers.Dense(units=2, activation='sigmoid')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = tf.convert_to_tensor(x)\n",
    "        x = self.sharedFC1(x)\n",
    "        x = self.sharedFC2(x)\n",
    "        \n",
    "        mu = self.muFC1(x)\n",
    "        mu = self.muFC2(mu)\n",
    "        mu = self.mu_out(mu)\n",
    "        \n",
    "        sigma = self.sigmaFC1(x)\n",
    "        sigma = self.sigmaFC2(sigma)\n",
    "        sigma = self.sigma_out(sigma)     \n",
    "        \n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/.venv/tensorflow-gpu/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 of 100 findished after 81 timesteps\n",
      "Episode 2 of 100 findished after 137 timesteps\n",
      "Episode 3 of 100 findished after 85 timesteps\n",
      "Episode 4 of 100 findished after 94 timesteps\n",
      "Episode 5 of 100 findished after 73 timesteps\n",
      "Episode 6 of 100 findished after 72 timesteps\n",
      "Episode 7 of 100 findished after 147 timesteps\n",
      "Episode 8 of 100 findished after 115 timesteps\n",
      "Episode 9 of 100 findished after 108 timesteps\n",
      "Episode 10 of 100 findished after 131 timesteps\n",
      "Episode 11 of 100 findished after 152 timesteps\n",
      "Episode 12 of 100 findished after 109 timesteps\n",
      "Episode 13 of 100 findished after 107 timesteps\n",
      "Episode 14 of 100 findished after 106 timesteps\n",
      "Episode 15 of 100 findished after 100 timesteps\n",
      "Episode 16 of 100 findished after 156 timesteps\n",
      "Episode 17 of 100 findished after 100 timesteps\n",
      "Episode 18 of 100 findished after 121 timesteps\n",
      "Episode 19 of 100 findished after 101 timesteps\n",
      "Episode 20 of 100 findished after 109 timesteps\n",
      "Episode 21 of 100 findished after 68 timesteps\n",
      "Episode 22 of 100 findished after 97 timesteps\n",
      "Episode 23 of 100 findished after 133 timesteps\n",
      "Episode 24 of 100 findished after 92 timesteps\n",
      "Episode 25 of 100 findished after 113 timesteps\n",
      "Episode 26 of 100 findished after 102 timesteps\n",
      "Episode 27 of 100 findished after 75 timesteps\n",
      "Episode 28 of 100 findished after 157 timesteps\n",
      "Episode 29 of 100 findished after 81 timesteps\n",
      "Episode 30 of 100 findished after 105 timesteps\n",
      "Episode 31 of 100 findished after 189 timesteps\n",
      "Episode 32 of 100 findished after 156 timesteps\n",
      "Episode 33 of 100 findished after 97 timesteps\n",
      "Episode 34 of 100 findished after 123 timesteps\n",
      "Episode 35 of 100 findished after 83 timesteps\n",
      "Episode 36 of 100 findished after 130 timesteps\n",
      "Episode 37 of 100 findished after 88 timesteps\n",
      "Episode 38 of 100 findished after 73 timesteps\n",
      "Episode 39 of 100 findished after 126 timesteps\n",
      "Episode 40 of 100 findished after 86 timesteps\n",
      "Episode 41 of 100 findished after 86 timesteps\n",
      "Episode 42 of 100 findished after 66 timesteps\n",
      "Episode 43 of 100 findished after 103 timesteps\n"
     ]
    }
   ],
   "source": [
    "# Prepare Tensorboard\n",
    "!rm -rf ./logs/\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "#%tensorboard --logdir logs/\n",
    "#tf.keras.backend.clear_session()\n",
    "# Initialize cart pole environment\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "# Initialize model, loss and optimizer\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "actor_optimizer = tf.keras.optimizers.Adam()\n",
    "critic_optimizer = tf.keras.optimizers.Adam()\n",
    "mse = tf.keras.losses.MSE\n",
    "weighted_sparse_ce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "# Initialize replay memory\n",
    "observations = []\n",
    "# Set hyperparameters\n",
    "discount = 0.95\n",
    "max_time_steps = 500\n",
    "num_episodes = 100\n",
    "\n",
    "step = 0\n",
    "# Run for agent and environment for num_episodes\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    #breakpoint()\n",
    "    \n",
    "    # Agent has 500 trials at max, if it does not fail beforehand\n",
    "    for t in range(max_time_steps):\n",
    "        env.render()\n",
    "        # Compute action\n",
    "        state = np.reshape(state, [1,8])\n",
    "        mu, sigma = actor(state)\n",
    "        \n",
    "        # sample two values from normal distribution\n",
    "        mainEngineAction = tf.random.normal((1,), mean=mu[0,0], stddev=sigma[0,0])\n",
    "        sideEngineAction = tf.random.normal((1,), mean=mu[0,1], stddev=sigma[0,1])\n",
    "        action = tf.concat([mainEngineAction, sideEngineAction], 0)\n",
    "        #        mainEngineAction = np.reshape(action, (2,))\n",
    "        # Execute action and store action, state and reward\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        observations.append((state, action, reward))\n",
    "        state = next_state\n",
    "        \n",
    "        # Interrupt the trial if the agent fails\n",
    "        if done:\n",
    "            break\n",
    "        step += 1\n",
    "        \n",
    "    print(f\"Episode {i_episode + 1} of {num_episodes} findished after {t+1} timesteps\")\n",
    "        \n",
    "    # Store losses temporary\n",
    "    losses = []\n",
    "    # Initialize variable for the estimated return\n",
    "    estimated_return = 0 if done else critic(next_state)\n",
    "    \n",
    "    # Iterate over taken actions and observed states and rewards\n",
    "    observations.reverse()\n",
    "    for state, action, reward in observations:\n",
    "        # Compute estimated return\n",
    "        estimated_return = discount * estimated_return + reward\n",
    "        # Compute state value\n",
    "        state_v = critic(state)\n",
    "    \n",
    "        # Compute gradients for the actor (policy gradient)\n",
    "        # Maximize the estimated return\n",
    "        with tf.GradientTape() as actor_tape:\n",
    "            mu, sigma = actor(state)\n",
    "            advantages = estimated_return - int(state_v)\n",
    "            advantages = tf.cast([[advantages]], tf.float32)\n",
    "            logprob = -tf.math.reduce_sum(tf.math.square(action - mu) / (2 * sigma), axis=1)\n",
    "            #breakpoint()\n",
    "            # Compute the actor loss (log part of the policy gradient)\n",
    "            actor_loss = weighted_sparse_ce(logprob, logprob, sample_weight=advantages)\n",
    "            # Compute gradient with respect to the parameters of the actor            \n",
    "            policy_gradients = actor_tape.gradient(actor_loss, actor.trainable_variables)\n",
    "\n",
    "        # Compute gradients for the critic\n",
    "        # minimize MSE for the state value function\n",
    "        with tf.GradientTape() as critic_tape:\n",
    "            state_v = critic(state)\n",
    "            # Compute the loss\n",
    "            critic_loss = mse(estimated_return, state_v)\n",
    "            # Compute the gradient\n",
    "            critic_gradients = critic_tape.gradient(critic_loss, critic.trainable_variables)\n",
    "            #breakpoint()\n",
    "            # Accumulate gradients\n",
    "            #critic_gradients.append(gradients)\n",
    "            \n",
    "        # Apply gradients.\n",
    "        actor_optimizer.apply_gradients(zip(policy_gradients, actor.trainable_variables))\n",
    "        critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
    "        losses.append(actor_loss)\n",
    "\n",
    "    observations = []\n",
    "\n",
    "    # Store summary statistics\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('?b', tf.reduce_mean(losses), step=step)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
