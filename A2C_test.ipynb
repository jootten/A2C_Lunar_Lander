{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C - CartPole\n",
    "Implementation of the A2C RL Algorithm for the OpenAI's Gym environment CartPole-V1 (not in parallel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Fuction Estimator\n",
    "class Critic(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(units=128, input_shape=[8,], activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(units=64, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(units=1, activation='relu')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Value Fuction Estimator (q-network)\n",
    "class Actor(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        # 64(share) -> 64(share) -> 32 -> 32 -> mu(tanh) [-1,1]\n",
    "        # 64(share) -> 64(share) -> 32 -> 32 -> sigma(sigmoid) [0,1]\n",
    "        self.sharedFC1 = tf.keras.layers.Dense(units=64, input_shape=[8,], activation='relu')\n",
    "        self.sharedFC2 = tf.keras.layers.Dense(units=64, activation='relu')\n",
    "        \n",
    "        self.muFC1 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        self.muFC2 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        \n",
    "        self.sigmaFC1 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        self.sigmaFC2 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        \n",
    "        \n",
    "        self.mu_out = tf.keras.layers.Dense(units=2, activation='tanh')\n",
    "        self.sigma_out = tf.keras.layers.Dense(units=2, activation='sigmoid')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = tf.convert_to_tensor(x)\n",
    "        x = self.sharedFC1(x)\n",
    "        x = self.sharedFC2(x)\n",
    "        \n",
    "        mu = self.muFC1(x)\n",
    "        mu = self.muFC2(mu)\n",
    "        mu = self.mu_out(mu)\n",
    "        \n",
    "        sigma = self.sigmaFC1(x)\n",
    "        sigma = self.sigmaFC2(sigma)\n",
    "        sigma = self.sigma_out(sigma)     \n",
    "        \n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/.venv/tensorflow-gpu/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 of 10000 findished after 87 timesteps\n",
      "Episode 2 of 10000 findished after 84 timesteps\n",
      "Episode 3 of 10000 findished after 63 timesteps\n",
      "Episode 4 of 10000 findished after 80 timesteps\n",
      "Episode 5 of 10000 findished after 80 timesteps\n",
      "Episode 6 of 10000 findished after 77 timesteps\n",
      "Episode 7 of 10000 findished after 65 timesteps\n",
      "Episode 8 of 10000 findished after 54 timesteps\n",
      "Episode 9 of 10000 findished after 76 timesteps\n",
      "Episode 10 of 10000 findished after 55 timesteps\n",
      "Episode 11 of 10000 findished after 84 timesteps\n",
      "Episode 12 of 10000 findished after 85 timesteps\n",
      "Episode 13 of 10000 findished after 61 timesteps\n",
      "Episode 14 of 10000 findished after 66 timesteps\n",
      "Episode 15 of 10000 findished after 89 timesteps\n",
      "Episode 16 of 10000 findished after 62 timesteps\n",
      "Episode 17 of 10000 findished after 58 timesteps\n",
      "Episode 18 of 10000 findished after 57 timesteps\n",
      "Episode 19 of 10000 findished after 77 timesteps\n",
      "Episode 20 of 10000 findished after 59 timesteps\n",
      "Episode 21 of 10000 findished after 57 timesteps\n",
      "Episode 22 of 10000 findished after 71 timesteps\n",
      "Episode 23 of 10000 findished after 55 timesteps\n",
      "Episode 24 of 10000 findished after 70 timesteps\n",
      "Episode 25 of 10000 findished after 66 timesteps\n",
      "Episode 26 of 10000 findished after 73 timesteps\n",
      "Episode 27 of 10000 findished after 56 timesteps\n",
      "Episode 28 of 10000 findished after 76 timesteps\n",
      "Episode 29 of 10000 findished after 70 timesteps\n",
      "Episode 30 of 10000 findished after 88 timesteps\n",
      "Episode 31 of 10000 findished after 64 timesteps\n"
     ]
    }
   ],
   "source": [
    "# Prepare Tensorboard\n",
    "!rm -rf ./logs/\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "#%tensorboard --logdir logs/\n",
    "#tf.keras.backend.clear_session()\n",
    "# Initialize cart pole environment\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "# Initialize model, loss and optimizer\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "actor_optimizer = tf.keras.optimizers.Adam()\n",
    "critic_optimizer = tf.keras.optimizers.Adam()\n",
    "mse = tf.keras.losses.MSE\n",
    "weighted_sparse_ce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "# Initialize replay memory\n",
    "observations = []\n",
    "# Set hyperparameters\n",
    "discount = 0.95\n",
    "max_time_steps = 500\n",
    "num_episodes = 10000\n",
    "\n",
    "# Store losses temporary\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "accum_reward = 0.\n",
    "\n",
    "step = 0\n",
    "# Run for agent and environment for num_episodes\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    #breakpoint()\n",
    "    \n",
    "    # Agent has 500 trials at max, if it does not fail beforehand\n",
    "    for t in range(max_time_steps):\n",
    "        env.render()\n",
    "        # Compute action\n",
    "        state = np.reshape(state, [1,8])\n",
    "        mu, sigma = actor(state)\n",
    "        \n",
    "        # sample two values from normal distribution\n",
    "        mainEngineAction = tf.random.normal((1,), mean=mu[0,0], stddev=sigma[0,0])\n",
    "        sideEngineAction = tf.random.normal((1,), mean=mu[0,1], stddev=sigma[0,1])\n",
    "        action = tf.concat([mainEngineAction, sideEngineAction], 0)\n",
    "        #        mainEngineAction = np.reshape(action, (2,))\n",
    "        # Execute action and store action, state and reward\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        observations.append((state, action, reward))\n",
    "        state = next_state\n",
    "        accum_reward += reward\n",
    "        # Interrupt the trial if the agent fails\n",
    "        if done:\n",
    "            break\n",
    "        step += 1\n",
    "        \n",
    "    print(f\"Episode {i_episode + 1} of {num_episodes} findished after {t+1} timesteps\")\n",
    "        \n",
    "    # Initialize variable for the estimated return\n",
    "    estimated_return = 0 if done else critic(next_state)\n",
    "    \n",
    "    # Iterate over taken actions and observed states and rewards\n",
    "    observations.reverse()\n",
    "    for state, action, reward in observations:\n",
    "        # Compute estimated return\n",
    "        estimated_return = discount * estimated_return + reward\n",
    "        # Compute state value\n",
    "        state_v = critic(state)\n",
    "    \n",
    "        # Compute gradients for the actor (policy gradient)\n",
    "        # Maximize the estimated return\n",
    "        with tf.GradientTape() as actor_tape:\n",
    "            mu, sigma = actor(state)\n",
    "            advantages = estimated_return - int(state_v)\n",
    "            advantages = tf.cast([[advantages]], tf.float32)\n",
    "            action_distribution = tfp.distributions.Normal(loc=mu, scale=sigma)\n",
    "            logprob = action_distribution.log_prob(action)\n",
    "            #breakpoint()\n",
    "            actor_loss = logprob * advantages\n",
    "            #breakpoint()\n",
    "            \n",
    "            # Compute the actor loss (log part of the policy gradient)\n",
    "            # Compute gradient with respect to the parameters of the actor            \n",
    "            policy_gradients = actor_tape.gradient(actor_loss, actor.trainable_variables)\n",
    "\n",
    "        # Compute gradients for the critic\n",
    "        # minimize MSE for the state value function\n",
    "        with tf.GradientTape() as critic_tape:\n",
    "            state_v = critic(state)\n",
    "            # Compute the loss\n",
    "            critic_loss = mse(estimated_return, state_v)\n",
    "            # Compute the gradient\n",
    "            critic_gradients = critic_tape.gradient(critic_loss, critic.trainable_variables)\n",
    "            #breakpoint()\n",
    "            # Accumulate gradients\n",
    "            #critic_gradients.append(gradients)\n",
    "            \n",
    "        # Apply gradients.\n",
    "        actor_optimizer.apply_gradients(zip(policy_gradients, actor.trainable_variables))\n",
    "        critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
    "        actor_losses.append(actor_loss)\n",
    "        critic_losses.append(critic_loss)\n",
    "\n",
    "    observations = []\n",
    "\n",
    "    # Store summary statistics\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('policy loss', tf.reduce_mean(actor_losses), step=step)\n",
    "        \n",
    "    # Store summary statistics\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('critic loss', tf.reduce_mean(critic_losses), step=step)\n",
    "    # Store summary statistics\n",
    "    \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('accumulated reward', accum_reward, step=step)\n",
    "    accum_reward = 0.\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
