{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C - CartPole\n",
    "Implementation of the A2C RL Algorithm for the OpenAI's Gym environment CartPole-V1 (not in parallel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Fuction Estimator\n",
    "class Critic(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(units=128, input_shape=[8,], activation='relu')\n",
    "        \n",
    "        self.Dropout = tf.keras.layers.Dropout(rate=0.2)\n",
    "\n",
    "        self.fc2 = tf.keras.layers.Dense(units=64, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(units=1, activation=None)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.Dropout(x, training=True)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Value Fuction Estimator (q-network)\n",
    "class Actor(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        # 64(share) -> 64(share) -> 32 -> 32 -> mu(tanh) [-1,1]\n",
    "        # 64(share) -> 64(share) -> 32 -> 32 -> sigma(sigmoid) [0,1]\n",
    "        self.sharedFC1 = tf.keras.layers.Dense(units=64, input_shape=[8,], activation='relu')\n",
    "        self.sharedFC2 = tf.keras.layers.Dense(units=64, activation='relu')\n",
    "        \n",
    "        self.sharedBatchNorm = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.muFC1 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        self.muFC2 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        \n",
    "        self.sigmaFC1 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        self.sigmaFC2 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        \n",
    "        \n",
    "        self.mu_out = tf.keras.layers.Dense(units=2, activation='tanh')\n",
    "        self.sigma_out = tf.keras.layers.Dense(units=2, activation='sigmoid')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = tf.convert_to_tensor(x)\n",
    "        x = self.sharedFC1(x)\n",
    "        x = self.sharedFC2(x)\n",
    "        \n",
    "        x = self.sharedBatchNorm(x, training=True)\n",
    "        \n",
    "        mu = self.muFC1(x)\n",
    "        mu = self.muFC2(mu)\n",
    "        mu = self.mu_out(mu)\n",
    "        \n",
    "        sigma = self.sigmaFC1(x)\n",
    "        sigma = self.sigmaFC2(sigma)\n",
    "        sigma = self.sigma_out(sigma)     \n",
    "        \n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/.venv/tensorflow-gpu/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 of 10000 finished after 166 timesteps\n",
      "Episode 2 of 10000 finished after 150 timesteps\n",
      "Episode 3 of 10000 finished after 165 timesteps\n",
      "Episode 4 of 10000 finished after 65 timesteps\n",
      "Episode 5 of 10000 finished after 71 timesteps\n",
      "Episode 6 of 10000 finished after 66 timesteps\n",
      "Episode 7 of 10000 finished after 80 timesteps\n",
      "Episode 8 of 10000 finished after 64 timesteps\n",
      "Episode 9 of 10000 finished after 79 timesteps\n",
      "Episode 10 of 10000 finished after 83 timesteps\n",
      "Episode 11 of 10000 finished after 51 timesteps\n",
      "Episode 12 of 10000 finished after 71 timesteps\n",
      "Episode 13 of 10000 finished after 50 timesteps\n",
      "Episode 14 of 10000 finished after 66 timesteps\n",
      "Episode 15 of 10000 finished after 57 timesteps\n",
      "Episode 16 of 10000 finished after 62 timesteps\n",
      "Episode 17 of 10000 finished after 69 timesteps\n",
      "Episode 18 of 10000 finished after 51 timesteps\n",
      "Episode 19 of 10000 finished after 51 timesteps\n",
      "Episode 20 of 10000 finished after 77 timesteps\n",
      "Episode 21 of 10000 finished after 73 timesteps\n",
      "Episode 22 of 10000 finished after 52 timesteps\n",
      "Episode 23 of 10000 finished after 77 timesteps\n",
      "Episode 24 of 10000 finished after 89 timesteps\n",
      "Episode 25 of 10000 finished after 59 timesteps\n",
      "Episode 26 of 10000 finished after 84 timesteps\n",
      "Episode 27 of 10000 finished after 54 timesteps\n",
      "Episode 28 of 10000 finished after 53 timesteps\n",
      "Episode 29 of 10000 finished after 68 timesteps\n",
      "Episode 30 of 10000 finished after 63 timesteps\n",
      "Episode 31 of 10000 finished after 79 timesteps\n",
      "Episode 32 of 10000 finished after 50 timesteps\n",
      "Episode 33 of 10000 finished after 81 timesteps\n",
      "Episode 34 of 10000 finished after 66 timesteps\n",
      "Episode 35 of 10000 finished after 61 timesteps\n",
      "Episode 36 of 10000 finished after 59 timesteps\n",
      "Episode 37 of 10000 finished after 59 timesteps\n",
      "Episode 38 of 10000 finished after 64 timesteps\n",
      "Episode 39 of 10000 finished after 63 timesteps\n",
      "Episode 40 of 10000 finished after 57 timesteps\n",
      "Episode 41 of 10000 finished after 53 timesteps\n",
      "Episode 42 of 10000 finished after 51 timesteps\n",
      "Episode 43 of 10000 finished after 61 timesteps\n",
      "Episode 44 of 10000 finished after 86 timesteps\n",
      "Episode 45 of 10000 finished after 53 timesteps\n",
      "Episode 46 of 10000 finished after 61 timesteps\n",
      "Episode 47 of 10000 finished after 63 timesteps\n",
      "Episode 48 of 10000 finished after 65 timesteps\n",
      "Episode 49 of 10000 finished after 65 timesteps\n",
      "Episode 50 of 10000 finished after 70 timesteps\n",
      "Episode 51 of 10000 finished after 59 timesteps\n",
      "Episode 52 of 10000 finished after 85 timesteps\n",
      "Episode 53 of 10000 finished after 76 timesteps\n",
      "Episode 54 of 10000 finished after 49 timesteps\n",
      "Episode 55 of 10000 finished after 78 timesteps\n",
      "Episode 56 of 10000 finished after 66 timesteps\n",
      "Episode 57 of 10000 finished after 73 timesteps\n",
      "Episode 58 of 10000 finished after 56 timesteps\n",
      "Episode 59 of 10000 finished after 52 timesteps\n",
      "Episode 60 of 10000 finished after 72 timesteps\n",
      "Episode 61 of 10000 finished after 56 timesteps\n",
      "Episode 62 of 10000 finished after 65 timesteps\n",
      "Episode 63 of 10000 finished after 85 timesteps\n",
      "Episode 64 of 10000 finished after 84 timesteps\n",
      "Episode 65 of 10000 finished after 60 timesteps\n",
      "Episode 66 of 10000 finished after 76 timesteps\n",
      "Episode 67 of 10000 finished after 78 timesteps\n",
      "Episode 68 of 10000 finished after 69 timesteps\n",
      "Episode 69 of 10000 finished after 80 timesteps\n",
      "Episode 70 of 10000 finished after 53 timesteps\n",
      "Episode 71 of 10000 finished after 68 timesteps\n",
      "Episode 72 of 10000 finished after 53 timesteps\n",
      "Episode 73 of 10000 finished after 76 timesteps\n",
      "Episode 74 of 10000 finished after 58 timesteps\n",
      "Episode 75 of 10000 finished after 71 timesteps\n",
      "Episode 76 of 10000 finished after 78 timesteps\n",
      "Episode 77 of 10000 finished after 71 timesteps\n",
      "Episode 78 of 10000 finished after 75 timesteps\n",
      "Episode 79 of 10000 finished after 55 timesteps\n",
      "Episode 80 of 10000 finished after 67 timesteps\n",
      "Episode 81 of 10000 finished after 60 timesteps\n",
      "Episode 82 of 10000 finished after 82 timesteps\n",
      "Episode 83 of 10000 finished after 74 timesteps\n",
      "Episode 84 of 10000 finished after 90 timesteps\n",
      "Episode 85 of 10000 finished after 56 timesteps\n",
      "Episode 86 of 10000 finished after 94 timesteps\n",
      "Episode 87 of 10000 finished after 66 timesteps\n",
      "Episode 88 of 10000 finished after 78 timesteps\n",
      "Episode 89 of 10000 finished after 70 timesteps\n",
      "Episode 90 of 10000 finished after 57 timesteps\n",
      "Episode 91 of 10000 finished after 57 timesteps\n",
      "Episode 92 of 10000 finished after 77 timesteps\n",
      "Episode 93 of 10000 finished after 70 timesteps\n",
      "Episode 94 of 10000 finished after 82 timesteps\n",
      "Episode 95 of 10000 finished after 60 timesteps\n",
      "Episode 96 of 10000 finished after 78 timesteps\n",
      "Episode 97 of 10000 finished after 83 timesteps\n",
      "Episode 98 of 10000 finished after 75 timesteps\n",
      "Episode 99 of 10000 finished after 77 timesteps\n",
      "Episode 100 of 10000 finished after 76 timesteps\n",
      "Episode 101 of 10000 finished after 58 timesteps\n",
      "Episode 102 of 10000 finished after 49 timesteps\n",
      "Episode 103 of 10000 finished after 86 timesteps\n",
      "Episode 104 of 10000 finished after 86 timesteps\n",
      "Episode 105 of 10000 finished after 66 timesteps\n",
      "Episode 106 of 10000 finished after 50 timesteps\n",
      "Episode 107 of 10000 finished after 59 timesteps\n",
      "Episode 108 of 10000 finished after 54 timesteps\n",
      "Episode 109 of 10000 finished after 54 timesteps\n",
      "Episode 110 of 10000 finished after 51 timesteps\n",
      "Episode 111 of 10000 finished after 50 timesteps\n",
      "Episode 112 of 10000 finished after 88 timesteps\n",
      "Episode 113 of 10000 finished after 56 timesteps\n",
      "Episode 114 of 10000 finished after 61 timesteps\n",
      "Episode 115 of 10000 finished after 57 timesteps\n",
      "Episode 116 of 10000 finished after 62 timesteps\n",
      "Episode 117 of 10000 finished after 65 timesteps\n",
      "Episode 118 of 10000 finished after 87 timesteps\n",
      "Episode 119 of 10000 finished after 99 timesteps\n",
      "Episode 120 of 10000 finished after 83 timesteps\n",
      "Episode 121 of 10000 finished after 89 timesteps\n",
      "Episode 122 of 10000 finished after 56 timesteps\n",
      "Episode 123 of 10000 finished after 79 timesteps\n",
      "Episode 124 of 10000 finished after 68 timesteps\n",
      "Episode 125 of 10000 finished after 75 timesteps\n",
      "Episode 126 of 10000 finished after 55 timesteps\n",
      "Episode 127 of 10000 finished after 59 timesteps\n",
      "Episode 128 of 10000 finished after 62 timesteps\n",
      "Episode 129 of 10000 finished after 86 timesteps\n",
      "Episode 130 of 10000 finished after 52 timesteps\n",
      "Episode 131 of 10000 finished after 84 timesteps\n",
      "Episode 132 of 10000 finished after 61 timesteps\n",
      "Episode 133 of 10000 finished after 75 timesteps\n",
      "Episode 134 of 10000 finished after 84 timesteps\n",
      "Episode 135 of 10000 finished after 69 timesteps\n",
      "Episode 136 of 10000 finished after 76 timesteps\n",
      "Episode 137 of 10000 finished after 67 timesteps\n",
      "Episode 138 of 10000 finished after 60 timesteps\n",
      "Episode 139 of 10000 finished after 70 timesteps\n",
      "Episode 140 of 10000 finished after 73 timesteps\n",
      "Episode 141 of 10000 finished after 71 timesteps\n",
      "Episode 142 of 10000 finished after 63 timesteps\n",
      "Episode 143 of 10000 finished after 65 timesteps\n",
      "Episode 144 of 10000 finished after 82 timesteps\n",
      "Episode 145 of 10000 finished after 89 timesteps\n",
      "Episode 146 of 10000 finished after 61 timesteps\n",
      "Episode 147 of 10000 finished after 56 timesteps\n",
      "Episode 148 of 10000 finished after 68 timesteps\n",
      "Episode 149 of 10000 finished after 49 timesteps\n",
      "Episode 150 of 10000 finished after 60 timesteps\n",
      "Episode 151 of 10000 finished after 69 timesteps\n",
      "Episode 152 of 10000 finished after 84 timesteps\n",
      "Episode 153 of 10000 finished after 65 timesteps\n",
      "Episode 154 of 10000 finished after 87 timesteps\n",
      "Episode 155 of 10000 finished after 62 timesteps\n",
      "Episode 156 of 10000 finished after 68 timesteps\n",
      "Episode 157 of 10000 finished after 67 timesteps\n",
      "Episode 158 of 10000 finished after 52 timesteps\n",
      "Episode 159 of 10000 finished after 63 timesteps\n",
      "Episode 160 of 10000 finished after 83 timesteps\n",
      "Episode 161 of 10000 finished after 87 timesteps\n",
      "Episode 162 of 10000 finished after 57 timesteps\n",
      "Episode 163 of 10000 finished after 61 timesteps\n",
      "Episode 164 of 10000 finished after 90 timesteps\n",
      "Episode 165 of 10000 finished after 87 timesteps\n",
      "Episode 166 of 10000 finished after 54 timesteps\n",
      "Episode 167 of 10000 finished after 78 timesteps\n",
      "Episode 168 of 10000 finished after 82 timesteps\n",
      "Episode 169 of 10000 finished after 68 timesteps\n",
      "Episode 170 of 10000 finished after 58 timesteps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 171 of 10000 finished after 60 timesteps\n",
      "Episode 172 of 10000 finished after 81 timesteps\n",
      "Episode 173 of 10000 finished after 86 timesteps\n",
      "Episode 174 of 10000 finished after 78 timesteps\n",
      "Episode 175 of 10000 finished after 66 timesteps\n",
      "Episode 176 of 10000 finished after 80 timesteps\n",
      "Episode 177 of 10000 finished after 62 timesteps\n",
      "Episode 178 of 10000 finished after 57 timesteps\n",
      "Episode 179 of 10000 finished after 51 timesteps\n",
      "Episode 180 of 10000 finished after 70 timesteps\n",
      "Episode 181 of 10000 finished after 85 timesteps\n",
      "Episode 182 of 10000 finished after 84 timesteps\n",
      "Episode 183 of 10000 finished after 48 timesteps\n",
      "Episode 184 of 10000 finished after 72 timesteps\n",
      "Episode 185 of 10000 finished after 83 timesteps\n",
      "Episode 186 of 10000 finished after 65 timesteps\n",
      "Episode 187 of 10000 finished after 60 timesteps\n",
      "Episode 188 of 10000 finished after 74 timesteps\n",
      "Episode 189 of 10000 finished after 71 timesteps\n",
      "Episode 190 of 10000 finished after 89 timesteps\n",
      "Episode 191 of 10000 finished after 57 timesteps\n",
      "Episode 192 of 10000 finished after 71 timesteps\n",
      "Episode 193 of 10000 finished after 69 timesteps\n",
      "Episode 194 of 10000 finished after 75 timesteps\n",
      "Episode 195 of 10000 finished after 79 timesteps\n",
      "Episode 196 of 10000 finished after 76 timesteps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1649b335a9d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# Compute the actor loss (log part of the policy gradient)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# Compute gradient with respect to the parameters of the actor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mpolicy_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Compute gradients for the critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/.venv/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_AddGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   (sx, rx, must_reduce_x), (sy, ry, must_reduce_y) = (\n\u001b[0;32m-> 1155\u001b[0;31m       SmartBroadcastGradientArgs(x, y, grad))\n\u001b[0m\u001b[1;32m   1156\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mskip_input_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[0mgx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py\u001b[0m in \u001b[0;36mSmartBroadcastGradientArgs\u001b[0;34m(x, y, grad)\u001b[0m\n\u001b[1;32m     93\u001b[0m       and isinstance(grad, ops.Tensor)):\n\u001b[1;32m     94\u001b[0m     \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0msy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_gradient_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m   \"\"\"\n\u001b[0;32m--> 545\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/tensorflow-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m   8217\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   8218\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8219\u001b[0;31m         tld.op_callbacks, input, \"out_type\", out_type)\n\u001b[0m\u001b[1;32m   8220\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare Tensorboard\n",
    "!rm -rf ./logs/\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "#%tensorboard --logdir logs/\n",
    "tf.keras.backend.clear_session()\n",
    "# Initialize cart pole environment\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "# Initialize model, loss and optimizer\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "mse = tf.keras.losses.MSE\n",
    "weighted_sparse_ce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "# Initialize replay memory\n",
    "observations = []\n",
    "# Set hyperparameters\n",
    "discount = 0.95\n",
    "max_time_steps = 500\n",
    "num_episodes = 10000\n",
    "\n",
    "# Store losses temporary\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "accum_reward = 0.\n",
    "\n",
    "step = 0\n",
    "# Run for agent and environment for num_episodes\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    #breakpoint()\n",
    "    \n",
    "    # Agent has 500 trials at max, if it does not fail beforehand\n",
    "    for t in range(max_time_steps):\n",
    "        env.render()\n",
    "        # Compute action\n",
    "        state = np.reshape(state, [1,8])\n",
    "        mu, sigma = actor(state)\n",
    "        \n",
    "        # sample two values from normal distribution\n",
    "        mainEngineAction = tf.random.normal((1,), mean=mu[0,0], stddev=sigma[0,0])\n",
    "        sideEngineAction = tf.random.normal((1,), mean=mu[0,1], stddev=sigma[0,1])\n",
    "        action = tf.concat([mainEngineAction, sideEngineAction], 0)\n",
    "        #        mainEngineAction = np.reshape(action, (2,))\n",
    "        # Execute action and store action, state and reward\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        observations.append((state, action, reward))\n",
    "        state = next_state\n",
    "        accum_reward += reward\n",
    "        # Interrupt the trial if the agent fails\n",
    "        if done:\n",
    "            break\n",
    "        step += 1\n",
    "        \n",
    "    print(f\"Episode {i_episode + 1} of {num_episodes} finished after {t+1} timesteps\")\n",
    "        \n",
    "    # Initialize variable for the estimated return\n",
    "    estimated_return = 0 if done else critic(next_state)\n",
    "    \n",
    "    # Iterate over taken actions and observed states and rewards\n",
    "    observations.reverse()\n",
    "    for state, action, reward in observations:\n",
    "        # Compute estimated return\n",
    "        estimated_return = discount * estimated_return + reward\n",
    "        # Compute state value\n",
    "        state_v = critic(state)\n",
    "    \n",
    "        # Compute gradients for the actor (policy gradient)\n",
    "        # Maximize the estimated return\n",
    "        with tf.GradientTape() as actor_tape:\n",
    "            mu, sigma = actor(state)\n",
    "            advantages = estimated_return - int(state_v)\n",
    "            advantages = tf.cast([[advantages]], tf.float32)\n",
    "            action_distribution = tfp.distributions.Normal(loc=mu, scale=sigma)\n",
    "            logprob = action_distribution.log_prob(action)\n",
    "            #breakpoint()\n",
    "            actor_loss = logprob * advantages\n",
    "            #breakpoint()\n",
    "            # Compute the actor loss (log part of the policy gradient)\n",
    "            # Compute gradient with respect to the parameters of the actor            \n",
    "            policy_gradients = actor_tape.gradient(actor_loss, actor.trainable_variables)\n",
    "\n",
    "        # Compute gradients for the critic\n",
    "        # minimize MSE for the state value function\n",
    "        with tf.GradientTape() as critic_tape:\n",
    "            state_v = critic(state)\n",
    "            # Compute the loss\n",
    "            critic_loss = mse(estimated_return, state_v)\n",
    "            # Compute the gradient\n",
    "            critic_gradients = critic_tape.gradient(critic_loss, critic.trainable_variables)\n",
    "            #breakpoint()\n",
    "            # Accumulate gradients\n",
    "            #critic_gradients.append(gradients)\n",
    "            \n",
    "        # Apply gradients.\n",
    "        actor_optimizer.apply_gradients(zip(policy_gradients, actor.trainable_variables))\n",
    "        critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
    "        actor_losses.append(actor_loss)\n",
    "        critic_losses.append(critic_loss)\n",
    "\n",
    "    observations = []\n",
    "\n",
    "    # Store summary statistics\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('policy loss', tf.reduce_mean(actor_losses), step=step)\n",
    "        \n",
    "        # Store summary statistics\n",
    "        tf.summary.scalar('critic loss', tf.reduce_mean(critic_losses), step=step)\n",
    "        \n",
    "        # Critic\n",
    "        #tf.summary.scalar('V(s)', state_v[0,0], step=step)\n",
    "        \n",
    "        # Actor\n",
    "        tf.summary.scalar('mu0', mu[0,0], step=step)\n",
    "        tf.summary.scalar('sigma0', sigma[0,0], step=step)\n",
    "        tf.summary.scalar('mu1', mu[0,1], step=step)\n",
    "        tf.summary.scalar('sigma1', sigma[0,1], step=step)\n",
    "        \n",
    "        # Accumulative reward\n",
    "        tf.summary.scalar(\"accumulative reward\", accum_reward, step=step)\n",
    "    \n",
    "    accum_reward = 0.\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
